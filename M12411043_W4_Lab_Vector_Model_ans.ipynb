{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18117881-1575-4349-95ba-9bc6b2cea328",
   "metadata": {},
   "source": [
    "# W4 Lab Exercise\n",
    "This is the lab exercise for MIS590: Information Retrieval. </br>\n",
    "In this lab, you will gain the following experience:</br>\n",
    "- Understand Vector Space Models (VSMs) for Information Retrieval.\n",
    "- Develop Practical Skills in Vector-Based Document Representation, Including TF-IDF, Word2Vec, and BERT.\n",
    "- Compare the Effectiveness of Different Term Weighting Schemes.\n",
    "- Enhance Analytical Thinking in Evaluating IR Models\n",
    "</br>\n",
    "\n",
    "**Note:** When you see a pencil icon ✏️ in this notebook, it's time for you to code!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffaa68b",
   "metadata": {},
   "source": [
    "# 1. Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111ef730",
   "metadata": {},
   "source": [
    "## 1.1 Install and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bae653b-f98f-4f96-826b-e8fdc579a542",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/pclo/anaconda3/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Users/pclo/anaconda3/lib/python3.11/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in /Users/pclo/anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/pclo/anaconda3/lib/python3.11/site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in /Users/pclo/anaconda3/lib/python3.11/site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: torch in /Users/pclo/anaconda3/lib/python3.11/site-packages (2.0.1)\n",
      "Requirement already satisfied: filelock in /Users/pclo/anaconda3/lib/python3.11/site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/pclo/anaconda3/lib/python3.11/site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in /Users/pclo/anaconda3/lib/python3.11/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/pclo/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/pclo/anaconda3/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/pclo/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/pclo/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /Users/pclo/anaconda3/lib/python3.11/site-packages (1.24.3)\n",
      "Requirement already satisfied: gensim in /Users/pclo/anaconda3/lib/python3.11/site-packages (4.3.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/pclo/anaconda3/lib/python3.11/site-packages (from gensim) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Users/pclo/anaconda3/lib/python3.11/site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/pclo/anaconda3/lib/python3.11/site-packages (from gensim) (5.2.1)\n",
      "Collecting FuzzyTM>=0.4.0 (from gensim)\n",
      "  Obtaining dependency information for FuzzyTM>=0.4.0 from https://files.pythonhosted.org/packages/2d/30/074bac7a25866a2807c1005c7852c0139ac22ba837871fc01f16df29b9dc/FuzzyTM-2.0.9-py3-none-any.whl.metadata\n",
      "  Downloading FuzzyTM-2.0.9-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: pandas in /Users/pclo/anaconda3/lib/python3.11/site-packages (from FuzzyTM>=0.4.0->gensim) (1.5.3)\n",
      "Collecting pyfume (from FuzzyTM>=0.4.0->gensim)\n",
      "  Obtaining dependency information for pyfume from https://files.pythonhosted.org/packages/ed/ea/a3b120e251145dcdb10777f2bc5f18b1496fd999d705a178c1b0ad947ce1/pyFUME-0.3.4-py3-none-any.whl.metadata\n",
      "  Downloading pyFUME-0.3.4-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/pclo/anaconda3/lib/python3.11/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/pclo/anaconda3/lib/python3.11/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2022.7)\n",
      "Collecting numpy>=1.18.5 (from gensim)\n",
      "  Obtaining dependency information for numpy>=1.18.5 from https://files.pythonhosted.org/packages/c0/bc/77635c657a3668cf652806210b8662e1aff84b818a55ba88257abf6637a8/numpy-1.24.4-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading numpy-1.24.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.6 kB)\n",
      "Collecting simpful==2.12.0 (from pyfume->FuzzyTM>=0.4.0->gensim)\n",
      "  Obtaining dependency information for simpful==2.12.0 from https://files.pythonhosted.org/packages/9d/0e/aebc2fb0b0f481994179b2ee2b8e6bbf0894d971594688c018375e7076ea/simpful-2.12.0-py3-none-any.whl.metadata\n",
      "  Downloading simpful-2.12.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting fst-pso==1.8.1 (from pyfume->FuzzyTM>=0.4.0->gensim)\n",
      "  Downloading fst-pso-1.8.1.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting miniful (from fst-pso==1.8.1->pyfume->FuzzyTM>=0.4.0->gensim)\n",
      "  Downloading miniful-0.0.6.tar.gz (2.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /Users/pclo/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas->FuzzyTM>=0.4.0->gensim) (1.16.0)\n",
      "Downloading FuzzyTM-2.0.9-py3-none-any.whl (31 kB)\n",
      "Downloading pyFUME-0.3.4-py3-none-any.whl (60 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.3/60.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.24.4-cp311-cp311-macosx_11_0_arm64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading simpful-2.12.0-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: fst-pso, miniful\n",
      "  Building wheel for fst-pso (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fst-pso: filename=fst_pso-1.8.1-py3-none-any.whl size=20430 sha256=02f926917a5fce0d96a8aefbc77af09c4927fb7c3b899ab7540a76ba1411b2f3\n",
      "  Stored in directory: /Users/pclo/Library/Caches/pip/wheels/69/f5/e5/18ad53fe1ed6b2af9fad05ec052e4acbac8e92441df44bad2e\n",
      "  Building wheel for miniful (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for miniful: filename=miniful-0.0.6-py3-none-any.whl size=3513 sha256=1adc1fedb7575e0cbec51ce8be9ce85c8e85c6ffb8205c670244af550856d391\n",
      "  Stored in directory: /Users/pclo/Library/Caches/pip/wheels/9d/ff/2f/afe4cd56f47de147407705626517d68bea0f3b74eb1fb168e6\n",
      "Successfully built fst-pso miniful\n",
      "Installing collected packages: numpy, simpful, miniful, fst-pso, pyfume, FuzzyTM\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.3\n",
      "    Uninstalling numpy-1.24.3:\n",
      "      Successfully uninstalled numpy-1.24.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tables 3.8.0 requires blosc2~=2.0.0, which is not installed.\n",
      "tables 3.8.0 requires cython>=0.29.21, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed FuzzyTM-2.0.9 fst-pso-1.8.1 miniful-0.0.6 numpy-1.24.4 pyfume-0.3.4 simpful-2.12.0\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement string (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for string\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: transformers in /Users/pclo/anaconda3/lib/python3.11/site-packages (4.29.2)\n",
      "Requirement already satisfied: filelock in /Users/pclo/anaconda3/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /Users/pclo/anaconda3/lib/python3.11/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/pclo/anaconda3/lib/python3.11/site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/pclo/anaconda3/lib/python3.11/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/pclo/anaconda3/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/pclo/anaconda3/lib/python3.11/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Users/pclo/anaconda3/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/pclo/anaconda3/lib/python3.11/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/pclo/anaconda3/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /Users/pclo/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/pclo/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/pclo/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/pclo/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/pclo/anaconda3/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/pclo/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: scikit-learn in /Users/pclo/anaconda3/lib/python3.11/site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/pclo/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/pclo/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/pclo/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/pclo/anaconda3/lib/python3.11/site-packages (from scikit-learn) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "# Install the necessary packages\n",
    "!pip install nltk\n",
    "!pip install torch\n",
    "!pip install numpy\n",
    "!pip install gensim\n",
    "!pip install string\n",
    "!pip install transformers\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53e49647",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import string\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecece4d5",
   "metadata": {},
   "source": [
    "## 1.2 Input: Query & Document Collections (Corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0ce3b5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"sleep deprivation\"\n",
    "\n",
    "corpus = [\n",
    "    \"Sleepless nights in the lab have become my new normal. I tried to fix the experiment setup, but the apparatus seems to have a mind of its own. My advisor says results are just around the corner, but the corner keeps moving. Coffee is my only true companion these days.\",\n",
    "    \"I thought grad school would be intellectually stimulating, but it's mostly paperwork and waiting for emails. The departmental printer jammed again, and now I'm late for a meeting. The cafeteria ran out of the good snacks, so I'm surviving on vending machine chips. Sleep has become a luxury I can no longer afford.\",\n",
    "    \"Writing the dissertation feels like climbing an endless mountain. Every time I finish a chapter, my supervisor suggests new revisions. The impostor syndrome is real, and I wonder if they made a mistake accepting me. Maybe I should have gone to clown college instead. I am utterly deprived of any semblance of a normal life.\",\n",
    "    \"My research data got corrupted, and now I have to start over. The lab mouse escaped, and we spent hours trying to find it. The grant proposal deadline is tomorrow, and the online submission portal is down. At least my pet cactus hasn't died yet.\",\n",
    "    \"The group meeting turned into a three-hour debate over font choices for the presentation. I'm pretty sure my colleague is stealing my lunch from the fridge. The photocopier is out to get me; it never works when I'm in a hurry. Is there a PhD in napping? Because I'd ace that.\",\n",
    "    \"I haven't seen the sun in days due to endless coding sessions. The simulation keeps crashing, and Stack Overflow doesn't have the answers. My roommate thinks I'm a ghost haunting the apartment. Instant noodles have become my primary food group.\",\n",
    "    \"Attending conferences sounded fun until I realized they involve a lot of awkward networking. I accidentally spilled coffee on a famous professor's shoes. My poster fell down twice during the session. Next time, I'll just send a cardboard cutout of myself.\",\n",
    "    \"The university gym membership was supposed to keep me healthy, but I've only used it once. I tried to attend a yoga class after staying up late for a deadline, but I fell asleep during the meditation. Maybe instead of the gym, my bed is more essential for keeping me healthy.\",\n",
    "    \"My teaching assistantship involves grading endless stacks of exams. Students keep emailing me for extensions with creative excuses. One claimed their dog sleeps on the laptop so they cannot use it for the exam. I was deprived of excuses for not completing my dissertation draft, and I might have got some good ones.\",\n",
    "    \"Group projects are the worst when you're the only one doing the work. My team members are as elusive as Bigfoot. The project is due next week, and I haven't heard from them. Perhaps I should just write a paper on the sociological implications of group work avoidance.\"\n",
    "]\n",
    "\n",
    "# Binary labels for the documents' relevancy to the query\n",
    "# Relevant ones: 1, 2, 5, 6, 8\n",
    "corpus_relevancy_label = [1, 1, 0, 0, 1, 1, 0, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c614242",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: sleep deprivation\n",
      "\n",
      "Document 1:\n",
      "Sleepless nights in the lab have become my new normal. I tried to fix the experiment setup, but the apparatus seems to have a mind of its own. My advisor says results are just around the corner, but the corner keeps moving. Coffee is my only true companion these days.\n",
      "\n",
      "Document 2:\n",
      "I thought grad school would be intellectually stimulating, but it's mostly paperwork and waiting for emails. The departmental printer jammed again, and now I'm late for a meeting. The cafeteria ran out of the good snacks, so I'm surviving on vending machine chips. Sleep has become a luxury I can no longer afford.\n",
      "\n",
      "Document 3:\n",
      "Writing the dissertation feels like climbing an endless mountain. Every time I finish a chapter, my supervisor suggests new revisions. The impostor syndrome is real, and I wonder if they made a mistake accepting me. Maybe I should have gone to clown college instead. I am utterly deprived of any semblance of a normal life.\n",
      "\n",
      "Document 4:\n",
      "My research data got corrupted, and now I have to start over. The lab mouse escaped, and we spent hours trying to find it. The grant proposal deadline is tomorrow, and the online submission portal is down. At least my pet cactus hasn't died yet.\n",
      "\n",
      "Document 5:\n",
      "The group meeting turned into a three-hour debate over font choices for the presentation. I'm pretty sure my colleague is stealing my lunch from the fridge. The photocopier is out to get me; it never works when I'm in a hurry. Is there a PhD in napping? Because I'd ace that.\n",
      "\n",
      "Document 6:\n",
      "I haven't seen the sun in days due to endless coding sessions. The simulation keeps crashing, and Stack Overflow doesn't have the answers. My roommate thinks I'm a ghost haunting the apartment. Instant noodles have become my primary food group.\n",
      "\n",
      "Document 7:\n",
      "Attending conferences sounded fun until I realized they involve a lot of awkward networking. I accidentally spilled coffee on a famous professor's shoes. My poster fell down twice during the session. Next time, I'll just send a cardboard cutout of myself.\n",
      "\n",
      "Document 8:\n",
      "The university gym membership was supposed to keep me healthy, but I've only used it once. I tried to attend a yoga class after staying up late for a deadline, but I fell asleep during the meditation. Maybe instead of the gym, my bed is more essential for keeping me healthy.\n",
      "\n",
      "Document 9:\n",
      "My teaching assistantship involves grading endless stacks of exams. Students keep emailing me for extensions with creative excuses. One claimed their dog sleeps on the laptop so they cannot use it for the exam. I was deprived of excuses for not completing my dissertation draft, and I might have got some good ones.\n",
      "\n",
      "Document 10:\n",
      "Group projects are the worst when you're the only one doing the work. My team members are as elusive as Bigfoot. The project is due next week, and I haven't heard from them. Perhaps I should just write a paper on the sociological implications of group work avoidance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Query: {query}\\n\")\n",
    "for idx, doc in enumerate(corpus):\n",
    "    print(f\"Document {idx+1}:\\n{doc}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e16ff55-2eb7-41d7-839d-73bb5272d04f",
   "metadata": {},
   "source": [
    "# 2. Vector Space Model: TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3088e0",
   "metadata": {},
   "source": [
    "## 2.1 Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c747d5",
   "metadata": {},
   "source": [
    "### Steps for textual data preprocessing\n",
    "1. Tokenization (= word segmentation)\n",
    "2. Punctualtion and non-alphabetic token removal\n",
    "3. Stopwords removal\n",
    "4. Lemmatization / stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0c16eb-1361-4977-a248-dd636a79fb5d",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30b23131-7e52-4097-8978-dd5b999ab25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/pclo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/pclo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/pclo/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/pclo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "538cd901",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Sentence:\n",
      "The graduate student was typing, procrastinating, questioning herself, and finally submitting the dissertation while dreaming about sleep.\n"
     ]
    }
   ],
   "source": [
    "# Initialize stopwords, lemmatizer, and punctuation list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "punctuation_table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "# We will use this sentence as example to showcase the different steps of data preprocessing\n",
    "example_sentence = \"The graduate student was typing, procrastinating, questioning herself, and finally submitting the dissertation while dreaming about sleep.\"\n",
    "print(f\"Example Sentence:\\n{example_sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42202e7",
   "metadata": {},
   "source": [
    "### What is tokenization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0eb5ba16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'graduate', 'student', 'was', 'typing', ',', 'procrastinating', ',', 'questioning', 'herself', ',', 'and', 'finally', 'submitting', 'the', 'dissertation', 'while', 'dreaming', 'about', 'sleep', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(example_sentence.lower())\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68961a99",
   "metadata": {},
   "source": [
    "### A quick removal of punctualtions and non-alphabetic words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77b14e1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'graduate', 'student', 'was', 'typing', 'procrastinating', 'questioning', 'herself', 'and', 'finally', 'submitting', 'the', 'dissertation', 'while', 'dreaming', 'about', 'sleep']\n"
     ]
    }
   ],
   "source": [
    "tokens_noPunc = [word.translate(punctuation_table) for word in tokens if word.isalpha()]\n",
    "print(tokens_noPunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a8030b",
   "metadata": {},
   "source": [
    "### What are stopwords?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "906e138c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['graduate', 'student', 'typing', 'procrastinating', 'questioning', 'finally', 'submitting', 'dissertation', 'dreaming', 'sleep']\n"
     ]
    }
   ],
   "source": [
    "tokens_noSW = [word for word in tokens_noPunc if word not in stop_words]\n",
    "print(tokens_noSW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b25394",
   "metadata": {},
   "source": [
    "### What is lemmatization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6427e0ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original\tLemmatized\n",
      "\n",
      "graduate\tgraduate\n",
      "student\tstudent\n",
      "typing\ttyping\n",
      "procrastinating\tprocrastinating\n",
      "questioning\tquestioning\n",
      "finally\tfinally\n",
      "submitting\tsubmitting\n",
      "dissertation\tdissertation\n",
      "dreaming\tdreaming\n",
      "sleep\tsleep\n"
     ]
    }
   ],
   "source": [
    "print(\"Original\\tLemmatized\\n\")\n",
    "\n",
    "# Here we use pre-stopword removal tokens\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens_noSW]\n",
    "for ori, lem in zip(tokens_noSW, lemmatized_tokens):\n",
    "    print(f\"{ori}\\t{lem}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0671810",
   "metadata": {},
   "source": [
    "### Observe the results above and discuss the following:\n",
    "- What is lemmatization?\n",
    "- I guess you cannot tell what lemmatization is from the results above. Let's try lemmatization in another way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675c21d4",
   "metadata": {},
   "source": [
    "### How about we tell the lemmatizer more information of the tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21a94a3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'DT'), ('graduate', 'NN'), ('student', 'NN'), ('was', 'VBD'), ('typing', 'VBG'), (',', ','), ('procrastinating', 'VBG'), (',', ','), ('questioning', 'VBG'), ('herself', 'PRP'), (',', ','), ('and', 'CC'), ('finally', 'RB'), ('submitting', 'VBG'), ('the', 'DT'), ('dissertation', 'NN'), ('while', 'IN'), ('dreaming', 'VBG'), ('about', 'RB'), ('sleep', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Part-of-Speech Tagging\n",
    "tagged_tokens = pos_tag(tokens)\n",
    "print(tagged_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4205ac6",
   "metadata": {},
   "source": [
    "### Then we do the punctuation, non-alphabetic tokens, and stopword removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6af0e29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'DT'), ('graduate', 'NN'), ('student', 'NN'), ('was', 'VBD'), ('typing', 'VBG'), ('procrastinating', 'VBG'), ('questioning', 'VBG'), ('herself', 'PRP'), ('and', 'CC'), ('finally', 'RB'), ('submitting', 'VBG'), ('the', 'DT'), ('dissertation', 'NN'), ('while', 'IN'), ('dreaming', 'VBG'), ('about', 'RB'), ('sleep', 'NN')]\n",
      "[('graduate', 'NN'), ('student', 'NN'), ('typing', 'VBG'), ('procrastinating', 'VBG'), ('questioning', 'VBG'), ('finally', 'RB'), ('submitting', 'VBG'), ('dissertation', 'NN'), ('dreaming', 'VBG'), ('sleep', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Remove punctuation and non-alphabetic tokens\n",
    "tagged_tokens_noPunc = [(word[0].translate(punctuation_table), word[1]) for word in tagged_tokens if word[0].isalpha()]\n",
    "print(tagged_tokens_noPunc)\n",
    "\n",
    "# Remove stopwords\n",
    "tagged_tokens_noSW = [(word[0], word[1]) for word in tagged_tokens_noPunc if word[0] not in stop_words]\n",
    "print(tagged_tokens_noSW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f9ffbf",
   "metadata": {},
   "source": [
    "### Take 2: what is lemmatization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1f708fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original\tLemmatized\n",
      "\n",
      "graduate\tgraduate\n",
      "student\tstudent\n",
      "typing\ttype\n",
      "procrastinating\tprocrastinate\n",
      "questioning\tquestion\n",
      "finally\tfinally\n",
      "submitting\tsubmit\n",
      "dissertation\tdissertation\n",
      "dreaming\tdream\n",
      "sleep\tsleep\n"
     ]
    }
   ],
   "source": [
    "# Convert treebank POS tags to wordnet POS tags so the lemmatizer can read them\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    else:\n",
    "        return wn.NOUN\n",
    "\n",
    "print(\"Original\\tLemmatized\\n\")\n",
    "tagged_tokens_lemmatized = [lemmatizer.lemmatize(token, get_wordnet_pos(tag)) for token, tag in tagged_tokens_noSW]\n",
    "for ori, lem in zip(tagged_tokens_noSW, tagged_tokens_lemmatized):\n",
    "    print(f\"{ori[0]}\\t{lem}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb33e82c",
   "metadata": {},
   "source": [
    "### Observe the results above and discuss the following:\n",
    "- What is lemmatization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc4fb82",
   "metadata": {},
   "source": [
    "### What is stemming?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2edec23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original\tStemmed\n",
      "\n",
      "graduate\tgraduat\n",
      "student\tstudent\n",
      "type\ttype\n",
      "procrastinate\tprocrastin\n",
      "question\tquestion\n",
      "finally\tfinal\n",
      "submit\tsubmit\n",
      "dissertation\tdissert\n",
      "dream\tdream\n",
      "sleep\tsleep\n"
     ]
    }
   ],
   "source": [
    "print(\"Original\\tStemmed\\n\")\n",
    "tokens_stemmed = [stemmer.stem(word) for word in tagged_tokens_lemmatized]\n",
    "for ori, stem in zip(tagged_tokens_lemmatized, tokens_stemmed):\n",
    "    print(f\"{ori}\\t{stem}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a2efd5",
   "metadata": {},
   "source": [
    "### Observe the results above and discuss the following:\n",
    "- What is stemming?\n",
    "- Why is stemming helpful in imporving TF-IDF performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9536305",
   "metadata": {},
   "source": [
    "### ✏️ Now let's preprocess the query and the documents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b3c6fe1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    # Step 1: # Convert to lowercase and tokenize text into words\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Step 2: Tag part-of-speech of the tokens\n",
    "    tokens = pos_tag(tokens)\n",
    "    \n",
    "    # Step 3: Remove punctuation and non-alphabetic tokens\n",
    "    tokens = [(word[0].translate(punctuation_table), word[1]) for word in tokens if word[0].isalpha()]\n",
    "    \n",
    "    # Step 4: Remove stopwords\n",
    "    tokens = [(word[0], word[1]) for word in tokens if word[0] not in stop_words]\n",
    "    \n",
    "    # Step 5: Lemmatize tokens\n",
    "    tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(tag)) for token, tag in tokens]\n",
    "    \n",
    "    # Step 6: Stem tokens\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d78d7fcb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: ['sleep', 'depriv']\n",
      "\n",
      "Document 1: ['sleepless', 'night', 'lab', 'becom', 'new', 'normal', 'tri', 'fix', 'experi', 'setup', 'apparatu', 'seem', 'mind', 'advisor', 'say', 'result', 'around', 'corner', 'corner', 'keep', 'move', 'coffe', 'true', 'companion', 'day']\n",
      "Document 2: ['think', 'grad', 'school', 'would', 'intellectu', 'stimul', 'mostli', 'paperwork', 'wait', 'email', 'department', 'printer', 'jam', 'late', 'meet', 'cafeteria', 'run', 'good', 'snack', 'surviv', 'vend', 'machin', 'chip', 'sleep', 'becom', 'luxuri', 'longer', 'afford']\n",
      "Document 3: ['write', 'dissert', 'feel', 'like', 'climb', 'endless', 'mountain', 'everi', 'time', 'finish', 'chapter', 'supervisor', 'suggest', 'new', 'revis', 'impostor', 'syndrom', 'real', 'wonder', 'make', 'mistak', 'accept', 'mayb', 'go', 'clown', 'colleg', 'instead', 'utterli', 'depriv', 'semblanc', 'normal', 'life']\n",
      "Document 4: ['research', 'data', 'get', 'corrupt', 'start', 'lab', 'mous', 'escap', 'spend', 'hour', 'tri', 'find', 'grant', 'propos', 'deadlin', 'tomorrow', 'onlin', 'submiss', 'portal', 'least', 'pet', 'cactu', 'die', 'yet']\n",
      "Document 5: ['group', 'meet', 'turn', 'debat', 'font', 'choic', 'present', 'pretti', 'sure', 'colleagu', 'steal', 'lunch', 'fridg', 'photocopi', 'get', 'never', 'work', 'hurri', 'phd', 'nap', 'ace']\n",
      "Document 6: ['see', 'sun', 'day', 'due', 'endless', 'cod', 'session', 'simul', 'keep', 'crash', 'stack', 'overflow', 'answer', 'roommat', 'think', 'ghost', 'haunt', 'apart', 'instant', 'noodl', 'becom', 'primari', 'food', 'group']\n",
      "Document 7: ['attend', 'confer', 'sound', 'fun', 'realiz', 'involv', 'lot', 'awkward', 'network', 'accident', 'spill', 'coffe', 'famou', 'professor', 'shoe', 'poster', 'fell', 'twice', 'session', 'next', 'time', 'send', 'cardboard', 'cutout']\n",
      "Document 8: ['univers', 'gym', 'membership', 'suppos', 'keep', 'healthi', 'use', 'tri', 'attend', 'yoga', 'class', 'stay', 'late', 'deadlin', 'fell', 'asleep', 'medit', 'mayb', 'instead', 'gym', 'bed', 'essenti', 'keep', 'healthi']\n",
      "Document 9: ['teach', 'assistantship', 'involv', 'grade', 'endless', 'stack', 'exam', 'student', 'keep', 'email', 'extens', 'creativ', 'excus', 'one', 'claim', 'dog', 'sleep', 'laptop', 'use', 'exam', 'depriv', 'excus', 'complet', 'dissert', 'draft', 'might', 'get', 'good', 'one']\n",
      "Document 10: ['group', 'project', 'bad', 'one', 'work', 'team', 'member', 'elus', 'bigfoot', 'project', 'due', 'next', 'week', 'hear', 'perhap', 'write', 'paper', 'sociolog', 'implic', 'group', 'work', 'avoid']\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing to each document in the corpus\n",
    "preprocessed_query = preprocess_text(query)\n",
    "print(f\"Query: {preprocessed_query}\\n\")\n",
    "\n",
    "preprocessed_corpus = [preprocess_text(doc) for doc in corpus]\n",
    "# Print preprocessed corpus\n",
    "for idx, doc in enumerate(preprocessed_corpus):\n",
    "    print(f\"Document {idx+1}: {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544cea0b",
   "metadata": {},
   "source": [
    "## ✏️ 2.2 Compute Term Frequency (TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "874163f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF for Document 1: {'sleepless': 0.04, 'night': 0.04, 'lab': 0.04, 'becom': 0.04, 'new': 0.04, 'normal': 0.04, 'tri': 0.04, 'fix': 0.04, 'experi': 0.04, 'setup': 0.04, 'apparatu': 0.04, 'seem': 0.04, 'mind': 0.04, 'advisor': 0.04, 'say': 0.04, 'result': 0.04, 'around': 0.04, 'corner': 0.08, 'keep': 0.04, 'move': 0.04, 'coffe': 0.04, 'true': 0.04, 'companion': 0.04, 'day': 0.04}\n",
      "\n",
      "TF for Document 2: {'think': 0.03571428571428571, 'grad': 0.03571428571428571, 'school': 0.03571428571428571, 'would': 0.03571428571428571, 'intellectu': 0.03571428571428571, 'stimul': 0.03571428571428571, 'mostli': 0.03571428571428571, 'paperwork': 0.03571428571428571, 'wait': 0.03571428571428571, 'email': 0.03571428571428571, 'department': 0.03571428571428571, 'printer': 0.03571428571428571, 'jam': 0.03571428571428571, 'late': 0.03571428571428571, 'meet': 0.03571428571428571, 'cafeteria': 0.03571428571428571, 'run': 0.03571428571428571, 'good': 0.03571428571428571, 'snack': 0.03571428571428571, 'surviv': 0.03571428571428571, 'vend': 0.03571428571428571, 'machin': 0.03571428571428571, 'chip': 0.03571428571428571, 'sleep': 0.03571428571428571, 'becom': 0.03571428571428571, 'luxuri': 0.03571428571428571, 'longer': 0.03571428571428571, 'afford': 0.03571428571428571}\n",
      "\n",
      "TF for Document 3: {'write': 0.03125, 'dissert': 0.03125, 'feel': 0.03125, 'like': 0.03125, 'climb': 0.03125, 'endless': 0.03125, 'mountain': 0.03125, 'everi': 0.03125, 'time': 0.03125, 'finish': 0.03125, 'chapter': 0.03125, 'supervisor': 0.03125, 'suggest': 0.03125, 'new': 0.03125, 'revis': 0.03125, 'impostor': 0.03125, 'syndrom': 0.03125, 'real': 0.03125, 'wonder': 0.03125, 'make': 0.03125, 'mistak': 0.03125, 'accept': 0.03125, 'mayb': 0.03125, 'go': 0.03125, 'clown': 0.03125, 'colleg': 0.03125, 'instead': 0.03125, 'utterli': 0.03125, 'depriv': 0.03125, 'semblanc': 0.03125, 'normal': 0.03125, 'life': 0.03125}\n",
      "\n",
      "TF for Document 4: {'research': 0.041666666666666664, 'data': 0.041666666666666664, 'get': 0.041666666666666664, 'corrupt': 0.041666666666666664, 'start': 0.041666666666666664, 'lab': 0.041666666666666664, 'mous': 0.041666666666666664, 'escap': 0.041666666666666664, 'spend': 0.041666666666666664, 'hour': 0.041666666666666664, 'tri': 0.041666666666666664, 'find': 0.041666666666666664, 'grant': 0.041666666666666664, 'propos': 0.041666666666666664, 'deadlin': 0.041666666666666664, 'tomorrow': 0.041666666666666664, 'onlin': 0.041666666666666664, 'submiss': 0.041666666666666664, 'portal': 0.041666666666666664, 'least': 0.041666666666666664, 'pet': 0.041666666666666664, 'cactu': 0.041666666666666664, 'die': 0.041666666666666664, 'yet': 0.041666666666666664}\n",
      "\n",
      "TF for Document 5: {'group': 0.047619047619047616, 'meet': 0.047619047619047616, 'turn': 0.047619047619047616, 'debat': 0.047619047619047616, 'font': 0.047619047619047616, 'choic': 0.047619047619047616, 'present': 0.047619047619047616, 'pretti': 0.047619047619047616, 'sure': 0.047619047619047616, 'colleagu': 0.047619047619047616, 'steal': 0.047619047619047616, 'lunch': 0.047619047619047616, 'fridg': 0.047619047619047616, 'photocopi': 0.047619047619047616, 'get': 0.047619047619047616, 'never': 0.047619047619047616, 'work': 0.047619047619047616, 'hurri': 0.047619047619047616, 'phd': 0.047619047619047616, 'nap': 0.047619047619047616, 'ace': 0.047619047619047616}\n",
      "\n",
      "TF for Document 6: {'see': 0.041666666666666664, 'sun': 0.041666666666666664, 'day': 0.041666666666666664, 'due': 0.041666666666666664, 'endless': 0.041666666666666664, 'cod': 0.041666666666666664, 'session': 0.041666666666666664, 'simul': 0.041666666666666664, 'keep': 0.041666666666666664, 'crash': 0.041666666666666664, 'stack': 0.041666666666666664, 'overflow': 0.041666666666666664, 'answer': 0.041666666666666664, 'roommat': 0.041666666666666664, 'think': 0.041666666666666664, 'ghost': 0.041666666666666664, 'haunt': 0.041666666666666664, 'apart': 0.041666666666666664, 'instant': 0.041666666666666664, 'noodl': 0.041666666666666664, 'becom': 0.041666666666666664, 'primari': 0.041666666666666664, 'food': 0.041666666666666664, 'group': 0.041666666666666664}\n",
      "\n",
      "TF for Document 7: {'attend': 0.041666666666666664, 'confer': 0.041666666666666664, 'sound': 0.041666666666666664, 'fun': 0.041666666666666664, 'realiz': 0.041666666666666664, 'involv': 0.041666666666666664, 'lot': 0.041666666666666664, 'awkward': 0.041666666666666664, 'network': 0.041666666666666664, 'accident': 0.041666666666666664, 'spill': 0.041666666666666664, 'coffe': 0.041666666666666664, 'famou': 0.041666666666666664, 'professor': 0.041666666666666664, 'shoe': 0.041666666666666664, 'poster': 0.041666666666666664, 'fell': 0.041666666666666664, 'twice': 0.041666666666666664, 'session': 0.041666666666666664, 'next': 0.041666666666666664, 'time': 0.041666666666666664, 'send': 0.041666666666666664, 'cardboard': 0.041666666666666664, 'cutout': 0.041666666666666664}\n",
      "\n",
      "TF for Document 8: {'univers': 0.041666666666666664, 'gym': 0.08333333333333333, 'membership': 0.041666666666666664, 'suppos': 0.041666666666666664, 'keep': 0.08333333333333333, 'healthi': 0.08333333333333333, 'use': 0.041666666666666664, 'tri': 0.041666666666666664, 'attend': 0.041666666666666664, 'yoga': 0.041666666666666664, 'class': 0.041666666666666664, 'stay': 0.041666666666666664, 'late': 0.041666666666666664, 'deadlin': 0.041666666666666664, 'fell': 0.041666666666666664, 'asleep': 0.041666666666666664, 'medit': 0.041666666666666664, 'mayb': 0.041666666666666664, 'instead': 0.041666666666666664, 'bed': 0.041666666666666664, 'essenti': 0.041666666666666664}\n",
      "\n",
      "TF for Document 9: {'teach': 0.034482758620689655, 'assistantship': 0.034482758620689655, 'involv': 0.034482758620689655, 'grade': 0.034482758620689655, 'endless': 0.034482758620689655, 'stack': 0.034482758620689655, 'exam': 0.06896551724137931, 'student': 0.034482758620689655, 'keep': 0.034482758620689655, 'email': 0.034482758620689655, 'extens': 0.034482758620689655, 'creativ': 0.034482758620689655, 'excus': 0.06896551724137931, 'one': 0.06896551724137931, 'claim': 0.034482758620689655, 'dog': 0.034482758620689655, 'sleep': 0.034482758620689655, 'laptop': 0.034482758620689655, 'use': 0.034482758620689655, 'depriv': 0.034482758620689655, 'complet': 0.034482758620689655, 'dissert': 0.034482758620689655, 'draft': 0.034482758620689655, 'might': 0.034482758620689655, 'get': 0.034482758620689655, 'good': 0.034482758620689655}\n",
      "\n",
      "TF for Document 10: {'group': 0.09090909090909091, 'project': 0.09090909090909091, 'bad': 0.045454545454545456, 'one': 0.045454545454545456, 'work': 0.09090909090909091, 'team': 0.045454545454545456, 'member': 0.045454545454545456, 'elus': 0.045454545454545456, 'bigfoot': 0.045454545454545456, 'due': 0.045454545454545456, 'next': 0.045454545454545456, 'week': 0.045454545454545456, 'hear': 0.045454545454545456, 'perhap': 0.045454545454545456, 'write': 0.045454545454545456, 'paper': 0.045454545454545456, 'sociolog': 0.045454545454545456, 'implic': 0.045454545454545456, 'avoid': 0.045454545454545456}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to compute term frequency (TF) for each document\n",
    "def compute_tf(doc):\n",
    "    \n",
    "    # Initialize the TF dictionary\n",
    "    tf_dict = {}\n",
    "    \n",
    "    # TODO\n",
    "    # Count the term frequency \n",
    "    for word in doc:\n",
    "        tf_dict[word] = tf_dict.get(word, 0) + 1\n",
    "    \n",
    "    # TODO\n",
    "    # Divide term counts by total number of terms in the document\n",
    "    total_terms = len(doc)\n",
    "    for word in tf_dict:\n",
    "        tf_dict[word] = tf_dict[word] / total_terms\n",
    "    \n",
    "    return tf_dict\n",
    "\n",
    "# Compute TF for each document in the corpus\n",
    "tf_corpus = [compute_tf(doc) for doc in preprocessed_corpus]\n",
    "\n",
    "# Print TF values for each document\n",
    "for idx, tf in enumerate(tf_corpus):\n",
    "    print(f\"TF for Document {idx+1}: {tf}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd3a5b8",
   "metadata": {},
   "source": [
    "## ✏️ 2.3 Compute Inverse Document Frequency (IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b72b4c5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF for Corpus:\n",
      "coffe: 2.6094379124341005\n",
      "advisor: 3.302585092994046\n",
      "fix: 3.302585092994046\n",
      "sleepless: 3.302585092994046\n",
      "night: 3.302585092994046\n",
      "tri: 2.203972804325936\n",
      "seem: 3.302585092994046\n",
      "companion: 3.302585092994046\n",
      "move: 3.302585092994046\n",
      "say: 3.302585092994046\n",
      "keep: 1.916290731874155\n",
      "setup: 3.302585092994046\n",
      "apparatu: 3.302585092994046\n",
      "day: 2.6094379124341005\n",
      "around: 3.302585092994046\n",
      "corner: 3.302585092994046\n",
      "becom: 2.203972804325936\n",
      "mind: 3.302585092994046\n",
      "normal: 2.6094379124341005\n",
      "experi: 3.302585092994046\n",
      "result: 3.302585092994046\n",
      "true: 3.302585092994046\n",
      "new: 2.6094379124341005\n",
      "lab: 2.6094379124341005\n",
      "mostli: 3.302585092994046\n",
      "longer: 3.302585092994046\n",
      "run: 3.302585092994046\n",
      "school: 3.302585092994046\n",
      "afford: 3.302585092994046\n",
      "would: 3.302585092994046\n",
      "intellectu: 3.302585092994046\n",
      "sleep: 2.6094379124341005\n",
      "luxuri: 3.302585092994046\n",
      "email: 2.6094379124341005\n",
      "stimul: 3.302585092994046\n",
      "wait: 3.302585092994046\n",
      "vend: 3.302585092994046\n",
      "department: 3.302585092994046\n",
      "grad: 3.302585092994046\n",
      "paperwork: 3.302585092994046\n",
      "meet: 2.6094379124341005\n",
      "jam: 3.302585092994046\n",
      "think: 2.6094379124341005\n",
      "printer: 3.302585092994046\n",
      "surviv: 3.302585092994046\n",
      "late: 2.6094379124341005\n",
      "good: 2.6094379124341005\n",
      "cafeteria: 3.302585092994046\n",
      "machin: 3.302585092994046\n",
      "chip: 3.302585092994046\n",
      "snack: 3.302585092994046\n",
      "go: 3.302585092994046\n",
      "mountain: 3.302585092994046\n",
      "feel: 3.302585092994046\n",
      "syndrom: 3.302585092994046\n",
      "everi: 3.302585092994046\n",
      "wonder: 3.302585092994046\n",
      "real: 3.302585092994046\n",
      "finish: 3.302585092994046\n",
      "suggest: 3.302585092994046\n",
      "dissert: 2.6094379124341005\n",
      "like: 3.302585092994046\n",
      "climb: 3.302585092994046\n",
      "supervisor: 3.302585092994046\n",
      "depriv: 2.6094379124341005\n",
      "write: 2.6094379124341005\n",
      "endless: 2.203972804325936\n",
      "mayb: 2.6094379124341005\n",
      "life: 3.302585092994046\n",
      "clown: 3.302585092994046\n",
      "chapter: 3.302585092994046\n",
      "time: 2.6094379124341005\n",
      "accept: 3.302585092994046\n",
      "make: 3.302585092994046\n",
      "revis: 3.302585092994046\n",
      "mistak: 3.302585092994046\n",
      "semblanc: 3.302585092994046\n",
      "instead: 2.6094379124341005\n",
      "utterli: 3.302585092994046\n",
      "impostor: 3.302585092994046\n",
      "colleg: 3.302585092994046\n",
      "propos: 3.302585092994046\n",
      "deadlin: 2.6094379124341005\n",
      "research: 3.302585092994046\n",
      "start: 3.302585092994046\n",
      "yet: 3.302585092994046\n",
      "cactu: 3.302585092994046\n",
      "get: 2.203972804325936\n",
      "data: 3.302585092994046\n",
      "portal: 3.302585092994046\n",
      "spend: 3.302585092994046\n",
      "mous: 3.302585092994046\n",
      "grant: 3.302585092994046\n",
      "onlin: 3.302585092994046\n",
      "least: 3.302585092994046\n",
      "escap: 3.302585092994046\n",
      "find: 3.302585092994046\n",
      "pet: 3.302585092994046\n",
      "tomorrow: 3.302585092994046\n",
      "hour: 3.302585092994046\n",
      "corrupt: 3.302585092994046\n",
      "submiss: 3.302585092994046\n",
      "die: 3.302585092994046\n",
      "font: 3.302585092994046\n",
      "never: 3.302585092994046\n",
      "photocopi: 3.302585092994046\n",
      "steal: 3.302585092994046\n",
      "lunch: 3.302585092994046\n",
      "group: 2.203972804325936\n",
      "ace: 3.302585092994046\n",
      "work: 2.6094379124341005\n",
      "fridg: 3.302585092994046\n",
      "choic: 3.302585092994046\n",
      "debat: 3.302585092994046\n",
      "hurri: 3.302585092994046\n",
      "turn: 3.302585092994046\n",
      "present: 3.302585092994046\n",
      "colleagu: 3.302585092994046\n",
      "phd: 3.302585092994046\n",
      "sure: 3.302585092994046\n",
      "pretti: 3.302585092994046\n",
      "nap: 3.302585092994046\n",
      "primari: 3.302585092994046\n",
      "stack: 2.6094379124341005\n",
      "simul: 3.302585092994046\n",
      "due: 2.6094379124341005\n",
      "roommat: 3.302585092994046\n",
      "ghost: 3.302585092994046\n",
      "noodl: 3.302585092994046\n",
      "see: 3.302585092994046\n",
      "apart: 3.302585092994046\n",
      "sun: 3.302585092994046\n",
      "crash: 3.302585092994046\n",
      "haunt: 3.302585092994046\n",
      "overflow: 3.302585092994046\n",
      "cod: 3.302585092994046\n",
      "session: 2.6094379124341005\n",
      "food: 3.302585092994046\n",
      "instant: 3.302585092994046\n",
      "answer: 3.302585092994046\n",
      "involv: 2.6094379124341005\n",
      "accident: 3.302585092994046\n",
      "spill: 3.302585092994046\n",
      "lot: 3.302585092994046\n",
      "confer: 3.302585092994046\n",
      "realiz: 3.302585092994046\n",
      "fun: 3.302585092994046\n",
      "shoe: 3.302585092994046\n",
      "send: 3.302585092994046\n",
      "network: 3.302585092994046\n",
      "professor: 3.302585092994046\n",
      "cardboard: 3.302585092994046\n",
      "awkward: 3.302585092994046\n",
      "twice: 3.302585092994046\n",
      "poster: 3.302585092994046\n",
      "sound: 3.302585092994046\n",
      "fell: 2.6094379124341005\n",
      "famou: 3.302585092994046\n",
      "next: 2.6094379124341005\n",
      "attend: 2.6094379124341005\n",
      "cutout: 3.302585092994046\n",
      "univers: 3.302585092994046\n",
      "stay: 3.302585092994046\n",
      "suppos: 3.302585092994046\n",
      "yoga: 3.302585092994046\n",
      "healthi: 3.302585092994046\n",
      "membership: 3.302585092994046\n",
      "bed: 3.302585092994046\n",
      "medit: 3.302585092994046\n",
      "use: 2.6094379124341005\n",
      "class: 3.302585092994046\n",
      "essenti: 3.302585092994046\n",
      "asleep: 3.302585092994046\n",
      "gym: 3.302585092994046\n",
      "exam: 3.302585092994046\n",
      "laptop: 3.302585092994046\n",
      "claim: 3.302585092994046\n",
      "draft: 3.302585092994046\n",
      "teach: 3.302585092994046\n",
      "grade: 3.302585092994046\n",
      "creativ: 3.302585092994046\n",
      "student: 3.302585092994046\n",
      "assistantship: 3.302585092994046\n",
      "extens: 3.302585092994046\n",
      "excus: 3.302585092994046\n",
      "one: 2.6094379124341005\n",
      "complet: 3.302585092994046\n",
      "might: 3.302585092994046\n",
      "dog: 3.302585092994046\n",
      "elus: 3.302585092994046\n",
      "team: 3.302585092994046\n",
      "member: 3.302585092994046\n",
      "hear: 3.302585092994046\n",
      "perhap: 3.302585092994046\n",
      "week: 3.302585092994046\n",
      "paper: 3.302585092994046\n",
      "project: 3.302585092994046\n",
      "avoid: 3.302585092994046\n",
      "sociolog: 3.302585092994046\n",
      "bigfoot: 3.302585092994046\n",
      "implic: 3.302585092994046\n",
      "bad: 3.302585092994046\n"
     ]
    }
   ],
   "source": [
    "# Function to compute inverse document frequency (IDF) for each term in the corpus\n",
    "def compute_idf(corpus):\n",
    "    \n",
    "    N = len(corpus)  # Total number of documents\n",
    "    \n",
    "    # Initialize the IDF dictionary\n",
    "    idf_dict = defaultdict(int)\n",
    "    \n",
    "    # TODO\n",
    "    # Count the number of documents containing each word\n",
    "    for doc in corpus:\n",
    "        for word in set(doc):  # Use set to count each word only once per document\n",
    "            idf_dict[word] += 1\n",
    "    \n",
    "    #TODO\n",
    "    # Compute IDF (logarithmic scale)\n",
    "    for word in idf_dict:\n",
    "        idf_dict[word] = math.log(N / (idf_dict[word])) + 1  # Smoothing by adding 1\n",
    "    \n",
    "    return idf_dict\n",
    "\n",
    "# Compute IDF for the corpus\n",
    "idf_dict = compute_idf(preprocessed_corpus)\n",
    "\n",
    "# Print IDF values\n",
    "print(\"IDF for Corpus:\")\n",
    "for word, idf in idf_dict.items():\n",
    "    print(f\"{word}: {idf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bface74",
   "metadata": {},
   "source": [
    "## ✏️ 2.4 Compute TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce20a060",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF for Document 1: {'sleepless': 0.13210340371976184, 'night': 0.13210340371976184, 'lab': 0.10437751649736403, 'becom': 0.08815891217303744, 'new': 0.10437751649736403, 'normal': 0.10437751649736403, 'tri': 0.08815891217303744, 'fix': 0.13210340371976184, 'experi': 0.13210340371976184, 'setup': 0.13210340371976184, 'apparatu': 0.13210340371976184, 'seem': 0.13210340371976184, 'mind': 0.13210340371976184, 'advisor': 0.13210340371976184, 'say': 0.13210340371976184, 'result': 0.13210340371976184, 'around': 0.13210340371976184, 'corner': 0.2642068074395237, 'keep': 0.0766516292749662, 'move': 0.13210340371976184, 'coffe': 0.10437751649736403, 'true': 0.13210340371976184, 'companion': 0.13210340371976184, 'day': 0.10437751649736403}\n",
      "\n",
      "TF-IDF for Document 2: {'think': 0.09319421115836073, 'grad': 0.1179494676069302, 'school': 0.1179494676069302, 'would': 0.1179494676069302, 'intellectu': 0.1179494676069302, 'stimul': 0.1179494676069302, 'mostli': 0.1179494676069302, 'paperwork': 0.1179494676069302, 'wait': 0.1179494676069302, 'email': 0.09319421115836073, 'department': 0.1179494676069302, 'printer': 0.1179494676069302, 'jam': 0.1179494676069302, 'late': 0.09319421115836073, 'meet': 0.09319421115836073, 'cafeteria': 0.1179494676069302, 'run': 0.1179494676069302, 'good': 0.09319421115836073, 'snack': 0.1179494676069302, 'surviv': 0.1179494676069302, 'vend': 0.1179494676069302, 'machin': 0.1179494676069302, 'chip': 0.1179494676069302, 'sleep': 0.09319421115836073, 'becom': 0.078713314440212, 'luxuri': 0.1179494676069302, 'longer': 0.1179494676069302, 'afford': 0.1179494676069302}\n",
      "\n",
      "TF-IDF for Document 3: {'write': 0.08154493476356564, 'dissert': 0.08154493476356564, 'feel': 0.10320578415606393, 'like': 0.10320578415606393, 'climb': 0.10320578415606393, 'endless': 0.0688741501351855, 'mountain': 0.10320578415606393, 'everi': 0.10320578415606393, 'time': 0.08154493476356564, 'finish': 0.10320578415606393, 'chapter': 0.10320578415606393, 'supervisor': 0.10320578415606393, 'suggest': 0.10320578415606393, 'new': 0.08154493476356564, 'revis': 0.10320578415606393, 'impostor': 0.10320578415606393, 'syndrom': 0.10320578415606393, 'real': 0.10320578415606393, 'wonder': 0.10320578415606393, 'make': 0.10320578415606393, 'mistak': 0.10320578415606393, 'accept': 0.10320578415606393, 'mayb': 0.08154493476356564, 'go': 0.10320578415606393, 'clown': 0.10320578415606393, 'colleg': 0.10320578415606393, 'instead': 0.08154493476356564, 'utterli': 0.10320578415606393, 'depriv': 0.08154493476356564, 'semblanc': 0.10320578415606393, 'normal': 0.08154493476356564, 'life': 0.10320578415606393}\n",
      "\n",
      "TF-IDF for Document 4: {'research': 0.13760771220808524, 'data': 0.13760771220808524, 'get': 0.09183220018024732, 'corrupt': 0.13760771220808524, 'start': 0.13760771220808524, 'lab': 0.10872657968475419, 'mous': 0.13760771220808524, 'escap': 0.13760771220808524, 'spend': 0.13760771220808524, 'hour': 0.13760771220808524, 'tri': 0.09183220018024732, 'find': 0.13760771220808524, 'grant': 0.13760771220808524, 'propos': 0.13760771220808524, 'deadlin': 0.10872657968475419, 'tomorrow': 0.13760771220808524, 'onlin': 0.13760771220808524, 'submiss': 0.13760771220808524, 'portal': 0.13760771220808524, 'least': 0.13760771220808524, 'pet': 0.13760771220808524, 'cactu': 0.13760771220808524, 'die': 0.13760771220808524, 'yet': 0.13760771220808524}\n",
      "\n",
      "TF-IDF for Document 5: {'group': 0.10495108592028266, 'meet': 0.12425894821114764, 'turn': 0.15726595680924027, 'debat': 0.15726595680924027, 'font': 0.15726595680924027, 'choic': 0.15726595680924027, 'present': 0.15726595680924027, 'pretti': 0.15726595680924027, 'sure': 0.15726595680924027, 'colleagu': 0.15726595680924027, 'steal': 0.15726595680924027, 'lunch': 0.15726595680924027, 'fridg': 0.15726595680924027, 'photocopi': 0.15726595680924027, 'get': 0.10495108592028266, 'never': 0.15726595680924027, 'work': 0.12425894821114764, 'hurri': 0.15726595680924027, 'phd': 0.15726595680924027, 'nap': 0.15726595680924027, 'ace': 0.15726595680924027}\n",
      "\n",
      "TF-IDF for Document 6: {'see': 0.13760771220808524, 'sun': 0.13760771220808524, 'day': 0.10872657968475419, 'due': 0.10872657968475419, 'endless': 0.09183220018024732, 'cod': 0.13760771220808524, 'session': 0.10872657968475419, 'simul': 0.13760771220808524, 'keep': 0.07984544716142312, 'crash': 0.13760771220808524, 'stack': 0.10872657968475419, 'overflow': 0.13760771220808524, 'answer': 0.13760771220808524, 'roommat': 0.13760771220808524, 'think': 0.10872657968475419, 'ghost': 0.13760771220808524, 'haunt': 0.13760771220808524, 'apart': 0.13760771220808524, 'instant': 0.13760771220808524, 'noodl': 0.13760771220808524, 'becom': 0.09183220018024732, 'primari': 0.13760771220808524, 'food': 0.13760771220808524, 'group': 0.09183220018024732}\n",
      "\n",
      "TF-IDF for Document 7: {'attend': 0.10872657968475419, 'confer': 0.13760771220808524, 'sound': 0.13760771220808524, 'fun': 0.13760771220808524, 'realiz': 0.13760771220808524, 'involv': 0.10872657968475419, 'lot': 0.13760771220808524, 'awkward': 0.13760771220808524, 'network': 0.13760771220808524, 'accident': 0.13760771220808524, 'spill': 0.13760771220808524, 'coffe': 0.10872657968475419, 'famou': 0.13760771220808524, 'professor': 0.13760771220808524, 'shoe': 0.13760771220808524, 'poster': 0.13760771220808524, 'fell': 0.10872657968475419, 'twice': 0.13760771220808524, 'session': 0.10872657968475419, 'next': 0.10872657968475419, 'time': 0.10872657968475419, 'send': 0.13760771220808524, 'cardboard': 0.13760771220808524, 'cutout': 0.13760771220808524}\n",
      "\n",
      "TF-IDF for Document 8: {'univers': 0.13760771220808524, 'gym': 0.2752154244161705, 'membership': 0.13760771220808524, 'suppos': 0.13760771220808524, 'keep': 0.15969089432284625, 'healthi': 0.2752154244161705, 'use': 0.10872657968475419, 'tri': 0.09183220018024732, 'attend': 0.10872657968475419, 'yoga': 0.13760771220808524, 'class': 0.13760771220808524, 'stay': 0.13760771220808524, 'late': 0.10872657968475419, 'deadlin': 0.10872657968475419, 'fell': 0.10872657968475419, 'asleep': 0.13760771220808524, 'medit': 0.13760771220808524, 'mayb': 0.10872657968475419, 'instead': 0.10872657968475419, 'bed': 0.13760771220808524, 'essenti': 0.13760771220808524}\n",
      "\n",
      "TF-IDF for Document 9: {'teach': 0.11388224458600159, 'assistantship': 0.11388224458600159, 'involv': 0.0899806176701414, 'grade': 0.11388224458600159, 'endless': 0.07599906221813572, 'stack': 0.0899806176701414, 'exam': 0.22776448917200318, 'student': 0.11388224458600159, 'keep': 0.06607899075428121, 'email': 0.0899806176701414, 'extens': 0.11388224458600159, 'creativ': 0.11388224458600159, 'excus': 0.22776448917200318, 'one': 0.1799612353402828, 'claim': 0.11388224458600159, 'dog': 0.11388224458600159, 'sleep': 0.0899806176701414, 'laptop': 0.11388224458600159, 'use': 0.0899806176701414, 'depriv': 0.0899806176701414, 'complet': 0.11388224458600159, 'dissert': 0.0899806176701414, 'draft': 0.11388224458600159, 'might': 0.11388224458600159, 'get': 0.07599906221813572, 'good': 0.0899806176701414}\n",
      "\n",
      "TF-IDF for Document 10: {'group': 0.20036116402963053, 'project': 0.3002350084540042, 'bad': 0.1501175042270021, 'one': 0.11861081420155002, 'work': 0.23722162840310004, 'team': 0.1501175042270021, 'member': 0.1501175042270021, 'elus': 0.1501175042270021, 'bigfoot': 0.1501175042270021, 'due': 0.11861081420155002, 'next': 0.11861081420155002, 'week': 0.1501175042270021, 'hear': 0.1501175042270021, 'perhap': 0.1501175042270021, 'write': 0.11861081420155002, 'paper': 0.1501175042270021, 'sociolog': 0.1501175042270021, 'implic': 0.1501175042270021, 'avoid': 0.1501175042270021}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to compute TF-IDF for a document\n",
    "def compute_tfidf(tf_doc, idf_dict):\n",
    "    \n",
    "    # Initialize TF-IDF dictionary\n",
    "    tfidf_dict = {}\n",
    "    \n",
    "    # TODO\n",
    "    # Multiply TF by corresponding IDF\n",
    "    for word, tf_value in tf_doc.items():\n",
    "        tfidf_dict[word] = tf_value * idf_dict.get(word, 0)  # Multiply TF by corresponding IDF\n",
    "        \n",
    "    return tfidf_dict\n",
    "\n",
    "# Compute TF-IDF for each document in the corpus\n",
    "tfidf_corpus = [compute_tfidf(tf, idf_dict) for tf in tf_corpus]\n",
    "\n",
    "# Print TF-IDF values for each document\n",
    "for idx, tfidf in enumerate(tfidf_corpus):\n",
    "    print(f\"TF-IDF for Document {idx+1}: {tfidf}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d96e40",
   "metadata": {},
   "source": [
    "## 2.5 The Implementaion of Information Retrieval System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2306f35a",
   "metadata": {},
   "source": [
    "### Measuring similarity: cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af4c5181",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to compute cosine similarity between two vectors\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = sum(vec1.get(word, 0) * vec2.get(word, 0) for word in vec1)\n",
    "    magnitude1 = math.sqrt(sum([value ** 2 for value in vec1.values()]))\n",
    "    magnitude2 = math.sqrt(sum([value ** 2 for value in vec2.values()]))\n",
    "    \n",
    "    if not magnitude1 or not magnitude2:\n",
    "        return 0.0\n",
    "    return dot_product / (magnitude1 * magnitude2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b21add",
   "metadata": {},
   "source": [
    "### Rank the documents using cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0779c1f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Rankings based on Query:\n",
      "Rank 1: Document 9 with score 0.20850879673278558\n",
      "Rank 2: Document 2 with score 0.11131520312033671\n",
      "Rank 3: Document 3 with score 0.10476487421443352\n",
      "Rank 4: Document 1 with score 0.0\n",
      "Rank 5: Document 4 with score 0.0\n",
      "Rank 6: Document 5 with score 0.0\n",
      "Rank 7: Document 6 with score 0.0\n",
      "Rank 8: Document 7 with score 0.0\n",
      "Rank 9: Document 8 with score 0.0\n",
      "Rank 10: Document 10 with score 0.0\n"
     ]
    }
   ],
   "source": [
    "# Compute TF for the query\n",
    "tf_query = compute_tf(preprocessed_query)\n",
    "\n",
    "# Compute TF-IDF for the query\n",
    "tfidf_query = compute_tfidf(tf_query, idf_dict)\n",
    "\n",
    "# Compute the cosine similarity of each documents to the query\n",
    "rankings = []\n",
    "for idx, tfidf_doc in enumerate(tfidf_corpus):\n",
    "    score = cosine_similarity(tfidf_doc, tfidf_query)\n",
    "    rankings.append((idx + 1, score))\n",
    "\n",
    "# Sort documents by similarity score in descending order\n",
    "rankings = sorted(rankings, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print document rankings\n",
    "print(\"Document Rankings based on Query:\")\n",
    "for rank, (doc_idx, score) in enumerate(rankings, start=1):\n",
    "    print(f\"Rank {rank}: Document {doc_idx} with score {score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716209d8-1197-4daf-891a-def8d48f0f19",
   "metadata": {},
   "source": [
    "### Observe the results above and discuss the following:\n",
    "- Are the highly ranked documents relevant to the query?\n",
    "- Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7d2fa2",
   "metadata": {},
   "source": [
    "# 3. Vector Space Model: Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c969039",
   "metadata": {},
   "source": [
    "## 3.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80bbdc9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb50280",
   "metadata": {},
   "source": [
    "## 3.2 Load Pre-trained Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6ac3b19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the pre-trained Google News Word2Vec model\n",
    "# This might take a while\n",
    "model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7290df8b-cd95-4078-9274-6b2e2256e0a8",
   "metadata": {},
   "source": [
    "### Let's observe a Word2Vec vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74f9c375-10cf-4dc5-b8ce-8e3835dd9fe6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.06445312, -0.16015625, -0.01208496,  0.13476562, -0.22949219,\n",
       "        0.16210938,  0.3046875 , -0.1796875 , -0.12109375,  0.25390625,\n",
       "       -0.01428223, -0.06396484, -0.08056641, -0.05688477, -0.19628906,\n",
       "        0.2890625 , -0.05151367,  0.14257812, -0.10498047, -0.04736328,\n",
       "       -0.34765625,  0.35742188,  0.265625  ,  0.00188446, -0.01586914,\n",
       "        0.00195312, -0.35546875,  0.22167969,  0.05761719,  0.15917969,\n",
       "        0.08691406, -0.0267334 , -0.04785156,  0.23925781, -0.05981445,\n",
       "        0.0378418 ,  0.17382812, -0.41796875,  0.2890625 ,  0.32617188,\n",
       "        0.02429199, -0.01647949, -0.06494141, -0.08886719,  0.07666016,\n",
       "       -0.15136719,  0.05249023, -0.04199219, -0.05419922,  0.00108337,\n",
       "       -0.20117188,  0.12304688,  0.09228516,  0.10449219, -0.00408936,\n",
       "       -0.04199219,  0.01409912, -0.02111816, -0.13476562, -0.24316406,\n",
       "        0.16015625, -0.06689453, -0.08984375, -0.07177734, -0.00595093,\n",
       "       -0.00482178, -0.00089264, -0.30664062, -0.0625    ,  0.07958984,\n",
       "       -0.00909424, -0.04492188,  0.09960938, -0.33398438, -0.3984375 ,\n",
       "        0.05541992, -0.06689453, -0.04467773,  0.11767578, -0.13964844,\n",
       "       -0.26367188,  0.17480469, -0.17382812, -0.40625   , -0.06738281,\n",
       "       -0.07617188,  0.09423828,  0.20996094, -0.16308594, -0.08691406,\n",
       "       -0.0534668 , -0.10351562, -0.07617188, -0.11083984, -0.03515625,\n",
       "       -0.14941406,  0.0378418 ,  0.38671875,  0.14160156, -0.2890625 ,\n",
       "       -0.16894531, -0.140625  , -0.04174805,  0.22753906,  0.24023438,\n",
       "       -0.01599121, -0.06787109,  0.21875   , -0.42382812, -0.5625    ,\n",
       "       -0.49414062, -0.3359375 ,  0.13378906,  0.01141357,  0.13671875,\n",
       "        0.0324707 ,  0.06835938, -0.27539062, -0.15917969,  0.00121307,\n",
       "        0.01208496, -0.0039978 ,  0.00442505, -0.04541016,  0.08642578,\n",
       "        0.09960938, -0.04296875, -0.11328125,  0.13867188,  0.41796875,\n",
       "       -0.28320312, -0.07373047, -0.11425781,  0.08691406, -0.02148438,\n",
       "        0.328125  , -0.07373047, -0.01348877,  0.17773438, -0.02624512,\n",
       "        0.13378906, -0.11132812, -0.12792969, -0.12792969,  0.18945312,\n",
       "       -0.13867188,  0.29882812, -0.07714844, -0.37695312, -0.10351562,\n",
       "        0.16992188, -0.10742188, -0.29882812,  0.00866699, -0.27734375,\n",
       "       -0.20996094, -0.1796875 , -0.19628906, -0.22167969,  0.08886719,\n",
       "       -0.27734375, -0.13964844,  0.15917969,  0.03637695,  0.03320312,\n",
       "       -0.08105469,  0.25390625, -0.08691406, -0.21289062, -0.18945312,\n",
       "       -0.22363281,  0.06542969, -0.16601562,  0.08837891, -0.359375  ,\n",
       "       -0.09863281,  0.35546875, -0.00741577,  0.19042969,  0.16992188,\n",
       "       -0.06005859, -0.20605469,  0.08105469,  0.12988281, -0.01135254,\n",
       "        0.33203125, -0.08691406,  0.27539062, -0.03271484,  0.12011719,\n",
       "       -0.0625    ,  0.1953125 , -0.10986328, -0.11767578,  0.20996094,\n",
       "        0.19921875,  0.02954102, -0.16015625,  0.00276184, -0.01367188,\n",
       "        0.03442383, -0.19335938,  0.00352478, -0.06542969, -0.05566406,\n",
       "        0.09423828,  0.29296875,  0.04052734, -0.09326172, -0.10107422,\n",
       "       -0.27539062,  0.04394531, -0.07275391,  0.13867188,  0.02380371,\n",
       "        0.13085938,  0.00236511, -0.2265625 ,  0.34765625,  0.13574219,\n",
       "        0.05224609,  0.18164062,  0.0402832 ,  0.23730469, -0.16992188,\n",
       "        0.10058594,  0.03833008,  0.10839844, -0.05615234, -0.00946045,\n",
       "        0.14550781, -0.30078125, -0.32226562,  0.18847656, -0.40234375,\n",
       "       -0.3125    , -0.08007812, -0.26757812,  0.16699219,  0.07324219,\n",
       "        0.06347656,  0.06591797,  0.17285156, -0.17773438,  0.00276184,\n",
       "       -0.05761719, -0.2265625 , -0.19628906,  0.09667969,  0.13769531,\n",
       "       -0.49414062, -0.27929688,  0.12304688, -0.30078125,  0.01293945,\n",
       "       -0.1875    , -0.20898438, -0.1796875 , -0.16015625, -0.03295898,\n",
       "        0.00976562,  0.25390625, -0.25195312,  0.00210571,  0.04296875,\n",
       "        0.01184082, -0.20605469,  0.24804688, -0.203125  , -0.17773438,\n",
       "        0.07275391,  0.04541016,  0.21679688, -0.2109375 ,  0.14550781,\n",
       "       -0.16210938,  0.20410156, -0.19628906, -0.35742188,  0.35742188,\n",
       "       -0.11962891,  0.35742188,  0.10351562,  0.07080078, -0.24707031,\n",
       "       -0.10449219, -0.19238281,  0.1484375 ,  0.00057983,  0.296875  ,\n",
       "       -0.12695312, -0.03979492,  0.13183594, -0.16601562,  0.125     ,\n",
       "        0.05126953, -0.14941406,  0.13671875, -0.02075195,  0.34375   ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['apple']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe8f96c-27ce-49c9-9264-1c97f26f0706",
   "metadata": {},
   "source": [
    "### Observe the results above and discuss the following:\n",
    "- What is the data type of this vector?\n",
    "- What is the dimensionality?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8907ef-3d24-414d-aa18-3a0ec0f65552",
   "metadata": {},
   "source": [
    "### Finding analogies using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9d2fec01-8c35-48e5-b30d-22ad8bb1146a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('apples', 0.720359742641449),\n",
       " ('pear', 0.6450697183609009),\n",
       " ('fruit', 0.6410146355628967),\n",
       " ('berry', 0.6302294731140137),\n",
       " ('pears', 0.6133960485458374),\n",
       " ('strawberry', 0.6058261394500732),\n",
       " ('peach', 0.6025872826576233),\n",
       " ('potato', 0.5960935354232788),\n",
       " ('grape', 0.5935865044593811),\n",
       " ('blueberry', 0.5866668224334717)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"apple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "23ec66fe-f37a-405b-bac3-a887d3987e60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Apple_AAPL', 0.7456986308097839),\n",
       " ('Apple_Nasdaq_AAPL', 0.7300410270690918),\n",
       " ('Apple_NASDAQ_AAPL', 0.7175089120864868),\n",
       " ('Apple_Computer', 0.7145973443984985),\n",
       " ('iPhone', 0.6924266219139099),\n",
       " ('Apple_NSDQ_AAPL', 0.6868603825569153),\n",
       " ('Steve_Jobs', 0.6758422255516052),\n",
       " ('iPad', 0.6580768823623657),\n",
       " ('Apple_nasdaq_AAPL', 0.6444970369338989),\n",
       " ('AAPL_PriceWatch_Alert', 0.6439753174781799)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"Apple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "923cc06a-89bf-4edb-b36b-24438da99a51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Microsoft', 0.457754522562027),\n",
       " ('Steve_Ballmer', 0.42643362283706665),\n",
       " ('Robert_Gates', 0.4092489182949066),\n",
       " ('Ballmer', 0.40724438428878784),\n",
       " ('Mullen', 0.4004097878932953),\n",
       " ('Chief_Executive_Steve_Ballmer', 0.3993479013442993),\n",
       " ('BlackBerry_maker', 0.39889541268348694),\n",
       " ('Apple_Nasdaq_AAPL', 0.3958131670951843),\n",
       " ('REDMOND_Wash._Microsoft', 0.390895277261734),\n",
       " ('McAfee', 0.38951435685157776)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['Gates', 'Apple'], negative=['Jobs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68db33da-9360-4962-9572-cfac6d4339b9",
   "metadata": {},
   "source": [
    "## 3.3 Compute Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c90e2df9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Notice here we only tokenize and lowercase the tokens:\n",
    "tokens = [word_tokenize(doc.lower()) for doc in corpus]\n",
    "query_tokens = word_tokenize(query.lower())\n",
    "\n",
    "# Function to compute the average word vector for a document or query\n",
    "def compute_avg_vector(words, model):\n",
    "    vectors = [model[word] for word in words if word in model]\n",
    "    if len(vectors) > 0:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)  # Return zero vector if no word in model\n",
    "\n",
    "# Compute average word vectors for each document\n",
    "doc_vectors = [compute_avg_vector(doc, model) for doc in tokens]\n",
    "\n",
    "# Compute average word vector for the query\n",
    "query_vector = compute_avg_vector(query_tokens, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099abfd5-768c-454b-996a-31de2d13e150",
   "metadata": {},
   "source": [
    "## 3.4 The Implementaion of Information Retrieval System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02991a5-6595-45c2-ac42-80290749ee68",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Measuring similarity: cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1d17ff6b-5234-4c22-83af-98cd3ba5cad9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    magnitude1 = np.linalg.norm(vec1)\n",
    "    magnitude2 = np.linalg.norm(vec2)\n",
    "    \n",
    "    if magnitude1 == 0 or magnitude2 == 0:\n",
    "        return 0  # Avoid division by zero\n",
    "    return dot_product / (magnitude1 * magnitude2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b060db3-c084-4524-a1e1-9f735f745862",
   "metadata": {},
   "source": [
    "### ✏️ Rank the documents using cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6c0b54d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Rankings based on Query:\n",
      "Rank 1: Document 9 with score 0.3664294183254242\n",
      "Rank 2: Document 8 with score 0.35598620772361755\n",
      "Rank 3: Document 6 with score 0.35288575291633606\n",
      "Rank 4: Document 3 with score 0.34057319164276123\n",
      "Rank 5: Document 2 with score 0.3265257775783539\n",
      "Rank 6: Document 1 with score 0.30051568150520325\n",
      "Rank 7: Document 5 with score 0.284969687461853\n",
      "Rank 8: Document 10 with score 0.27993154525756836\n",
      "Rank 9: Document 4 with score 0.26217684149742126\n",
      "Rank 10: Document 7 with score 0.24750055372714996\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "# Rank documents based on similarity to the query\n",
    "rankings = []\n",
    "for idx, doc_vector in enumerate(doc_vectors):\n",
    "    score = cosine_similarity(doc_vector, query_vector)\n",
    "    rankings.append((idx + 1, score))\n",
    "\n",
    "# TODO\n",
    "# Sort documents by similarity score in descending order\n",
    "rankings = sorted(rankings, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print document rankings\n",
    "print(\"Document Rankings based on Query:\")\n",
    "for rank, (doc_idx, score) in enumerate(rankings, start=1):\n",
    "    print(f\"Rank {rank}: Document {doc_idx} with score {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8fe449-63c4-4c6e-906d-410df096d098",
   "metadata": {},
   "source": [
    "### Observe the results above and discuss the following:\n",
    "- How are the results using Word2Vec different from those using TF-IDF?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417282dc-3699-4371-9eef-8add77f031ff",
   "metadata": {},
   "source": [
    "### How about we learn our own word2vec model with the corpus?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc8fe76-697d-4de8-81bf-8c57473843f4",
   "metadata": {},
   "source": [
    "## 3.5 Train Word2Vec Model from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa51ff04-f8bb-4aac-a7e1-58e986a34bb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train Word2Vec on the corpus\n",
    "model_corpus = Word2Vec(sentences=tokens, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46432dde-4e00-44b3-a6c1-02bec6b9b567",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to compute the average word vector for a document or query\n",
    "def compute_avg_vector(words, model):\n",
    "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    if len(vectors) > 0:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)  # Return zero vector if no word in model\n",
    "\n",
    "# Compute average word vectors for each document\n",
    "doc_vectors = [compute_avg_vector(doc, model_corpus) for doc in tokens]\n",
    "\n",
    "# Compute average word vector for the query\n",
    "query_vector = compute_avg_vector(query_tokens, model_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d4883ff-a8f6-4f98-b2c1-680ee0415359",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Rankings based on Query:\n",
      "Rank 1: Document 9 with score 0.2207733690738678\n",
      "Rank 2: Document 5 with score 0.2106305956840515\n",
      "Rank 3: Document 2 with score 0.18398788571357727\n",
      "Rank 4: Document 6 with score 0.12987695634365082\n",
      "Rank 5: Document 7 with score 0.1196761205792427\n",
      "Rank 6: Document 4 with score 0.10431124269962311\n",
      "Rank 7: Document 1 with score 0.08538194000720978\n",
      "Rank 8: Document 3 with score 0.06317424029111862\n",
      "Rank 9: Document 10 with score 0.05981253460049629\n",
      "Rank 10: Document 8 with score 0.022469740360975266\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    magnitude1 = np.linalg.norm(vec1)\n",
    "    magnitude2 = np.linalg.norm(vec2)\n",
    "    \n",
    "    if magnitude1 == 0 or magnitude2 == 0:\n",
    "        return 0  # Avoid division by zero\n",
    "    return dot_product / (magnitude1 * magnitude2)\n",
    "\n",
    "# Rank documents based on similarity to the query\n",
    "rankings = []\n",
    "for idx, doc_vector in enumerate(doc_vectors):\n",
    "    score = cosine_similarity(doc_vector, query_vector)\n",
    "    rankings.append((idx + 1, score))\n",
    "\n",
    "# Sort documents by similarity score in descending order\n",
    "rankings = sorted(rankings, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print document rankings\n",
    "print(\"Document Rankings based on Query:\")\n",
    "for rank, (doc_idx, score) in enumerate(rankings, start=1):\n",
    "    print(f\"Rank {rank}: Document {doc_idx} with score {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc1fab1-80a6-4c82-abb5-90b664f9b78b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Observe the results above and discuss the following:\n",
    "- How are the results using self-trained Word2Vec different from those using pre-trained Word2Vec?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172430c1-cfc7-4857-b73e-046a0ff62b58",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Vector Space Model: BERT\n",
    "This is not how a BERT model is normally used, but we can see how contextualized embeddings are helpful in matching queries and documents beyond just words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5be2c3d-d3ec-42c5-8d05-158d70e20d6e",
   "metadata": {},
   "source": [
    "## 4.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "79a51e70-21d0-4ac7-bb4d-c5f9ae8a5b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bdb588-c151-4742-8c7c-c6c483b0c5f6",
   "metadata": {},
   "source": [
    "## 4.2 Load Pre-trained BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "61244e77-b91a-49af-badf-e2396e44f375",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to generate BERT embeddings for a given text\n",
    "def get_bert_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model_bert(**inputs)\n",
    "    # The [CLS] token embedding is typically used as the sentence representation\n",
    "    return outputs.last_hidden_state[:, 0, :]  # Return the embedding for the [CLS] token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29123a13-d710-4e79-bbf2-e704f81f0db5",
   "metadata": {},
   "source": [
    "## 4.3 Compute BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7418ceb4-412b-4f8e-94bd-66ee314e2766",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute BERT embeddings for the query\n",
    "query_embedding = get_bert_embedding(query)\n",
    "\n",
    "# Compute BERT embeddings for each document in the corpus\n",
    "corpus_embeddings = [get_bert_embedding(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3cd6f0-f535-4349-ae55-dc7038fbefa7",
   "metadata": {},
   "source": [
    "## 4.4 The Implementaion of Information Retrieval System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a879879f-40d7-4657-ba07-d6d022ce78b1",
   "metadata": {},
   "source": [
    "### Measuring similarity: cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "35665c91-5f13-4073-b800-edc0304e2c4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to compute cosine similarity between two vectors\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    vec1 = vec1.numpy()\n",
    "    vec2 = vec2.numpy()\n",
    "    dot_product = np.dot(vec1, vec2.T)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    \n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0.0\n",
    "    return dot_product / (norm1 * norm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f13b45b-d635-4db5-a8f5-80c9ea0b80ba",
   "metadata": {},
   "source": [
    "### Rank the documents using cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aa483b8d-66c8-462f-ae6b-46aa4c39fc6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Rankings based on BERT embeddings:\n",
      "Rank 1: Document 2 with score 0.8103362917900085\n",
      "Rank 2: Document 6 with score 0.7880856394767761\n",
      "Rank 3: Document 5 with score 0.7864925861358643\n",
      "Rank 4: Document 3 with score 0.7857744693756104\n",
      "Rank 5: Document 1 with score 0.7844796776771545\n",
      "Rank 6: Document 10 with score 0.7754186987876892\n",
      "Rank 7: Document 7 with score 0.7519574165344238\n",
      "Rank 8: Document 8 with score 0.7409095764160156\n",
      "Rank 9: Document 4 with score 0.739783763885498\n",
      "Rank 10: Document 9 with score 0.7047692537307739\n"
     ]
    }
   ],
   "source": [
    "# Rank documents based on similarity to the query\n",
    "rankings = []\n",
    "for idx, doc_embedding in enumerate(corpus_embeddings):\n",
    "    score = cosine_similarity(query_embedding[0], doc_embedding[0])\n",
    "    rankings.append((idx + 1, score))\n",
    "\n",
    "# Sort documents by similarity score in descending order\n",
    "rankings = sorted(rankings, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print document rankings\n",
    "print(\"Document Rankings based on BERT embeddings:\")\n",
    "for rank, (doc_idx, score) in enumerate(rankings, start=1):\n",
    "    print(f\"Rank {rank}: Document {doc_idx} with score {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9989f0-7148-4ba2-b804-2e5dbac0fbaf",
   "metadata": {},
   "source": [
    "### Observe the results above and discuss the following:\n",
    "- How are the results using contextualized word embeddings (BERT) different from those using Word2Vec?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a036dbdc",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5acd23-0d29-4c39-8cd8-29dcebea5d89",
   "metadata": {},
   "source": [
    "## Part 1: Implement Bigram TF-IDF\n",
    "Using the same query and corpus, implement your own information retrival system base on bigram TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "185a2ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram TF-IDF Document ranking based on query similarity:\n",
      " [1 9 8 7 6 5 4 3 2 0]\n",
      "Document 2: I thought grad school would be intellectually stimulating, but it's mostly paperwork and waiting for emails. The departmental printer jammed again, and now I'm late for a meeting. The cafeteria ran out of the good snacks, so I'm surviving on vending machine chips. Sleep has become a luxury I can no longer afford. - Similarity: 0.1589\n",
      "Document 10: Group projects are the worst when you're the only one doing the work. My team members are as elusive as Bigfoot. The project is due next week, and I haven't heard from them. Perhaps I should just write a paper on the sociological implications of group work avoidance. - Similarity: 0.0000\n",
      "Document 9: My teaching assistantship involves grading endless stacks of exams. Students keep emailing me for extensions with creative excuses. One claimed their dog sleeps on the laptop so they cannot use it for the exam. I was deprived of excuses for not completing my dissertation draft, and I might have got some good ones. - Similarity: 0.0000\n",
      "Document 8: The university gym membership was supposed to keep me healthy, but I've only used it once. I tried to attend a yoga class after staying up late for a deadline, but I fell asleep during the meditation. Maybe instead of the gym, my bed is more essential for keeping me healthy. - Similarity: 0.0000\n",
      "Document 7: Attending conferences sounded fun until I realized they involve a lot of awkward networking. I accidentally spilled coffee on a famous professor's shoes. My poster fell down twice during the session. Next time, I'll just send a cardboard cutout of myself. - Similarity: 0.0000\n",
      "Document 6: I haven't seen the sun in days due to endless coding sessions. The simulation keeps crashing, and Stack Overflow doesn't have the answers. My roommate thinks I'm a ghost haunting the apartment. Instant noodles have become my primary food group. - Similarity: 0.0000\n",
      "Document 5: The group meeting turned into a three-hour debate over font choices for the presentation. I'm pretty sure my colleague is stealing my lunch from the fridge. The photocopier is out to get me; it never works when I'm in a hurry. Is there a PhD in napping? Because I'd ace that. - Similarity: 0.0000\n",
      "Document 4: My research data got corrupted, and now I have to start over. The lab mouse escaped, and we spent hours trying to find it. The grant proposal deadline is tomorrow, and the online submission portal is down. At least my pet cactus hasn't died yet. - Similarity: 0.0000\n",
      "Document 3: Writing the dissertation feels like climbing an endless mountain. Every time I finish a chapter, my supervisor suggests new revisions. The impostor syndrome is real, and I wonder if they made a mistake accepting me. Maybe I should have gone to clown college instead. I am utterly deprived of any semblance of a normal life. - Similarity: 0.0000\n",
      "Document 1: Sleepless nights in the lab have become my new normal. I tried to fix the experiment setup, but the apparatus seems to have a mind of its own. My advisor says results are just around the corner, but the corner keeps moving. Coffee is my only true companion these days. - Similarity: 0.0000\n",
      "\n",
      "Bigram TF-IDF Document ranking based on query similarity:\n",
      " [9 8 7 6 5 4 3 2 1 0]\n",
      "Document 10: Group projects are the worst when you're the only one doing the work. My team members are as elusive as Bigfoot. The project is due next week, and I haven't heard from them. Perhaps I should just write a paper on the sociological implications of group work avoidance. - Similarity: 0.0000\n",
      "Document 9: My teaching assistantship involves grading endless stacks of exams. Students keep emailing me for extensions with creative excuses. One claimed their dog sleeps on the laptop so they cannot use it for the exam. I was deprived of excuses for not completing my dissertation draft, and I might have got some good ones. - Similarity: 0.0000\n",
      "Document 8: The university gym membership was supposed to keep me healthy, but I've only used it once. I tried to attend a yoga class after staying up late for a deadline, but I fell asleep during the meditation. Maybe instead of the gym, my bed is more essential for keeping me healthy. - Similarity: 0.0000\n",
      "Document 7: Attending conferences sounded fun until I realized they involve a lot of awkward networking. I accidentally spilled coffee on a famous professor's shoes. My poster fell down twice during the session. Next time, I'll just send a cardboard cutout of myself. - Similarity: 0.0000\n",
      "Document 6: I haven't seen the sun in days due to endless coding sessions. The simulation keeps crashing, and Stack Overflow doesn't have the answers. My roommate thinks I'm a ghost haunting the apartment. Instant noodles have become my primary food group. - Similarity: 0.0000\n",
      "Document 5: The group meeting turned into a three-hour debate over font choices for the presentation. I'm pretty sure my colleague is stealing my lunch from the fridge. The photocopier is out to get me; it never works when I'm in a hurry. Is there a PhD in napping? Because I'd ace that. - Similarity: 0.0000\n",
      "Document 4: My research data got corrupted, and now I have to start over. The lab mouse escaped, and we spent hours trying to find it. The grant proposal deadline is tomorrow, and the online submission portal is down. At least my pet cactus hasn't died yet. - Similarity: 0.0000\n",
      "Document 3: Writing the dissertation feels like climbing an endless mountain. Every time I finish a chapter, my supervisor suggests new revisions. The impostor syndrome is real, and I wonder if they made a mistake accepting me. Maybe I should have gone to clown college instead. I am utterly deprived of any semblance of a normal life. - Similarity: 0.0000\n",
      "Document 2: I thought grad school would be intellectually stimulating, but it's mostly paperwork and waiting for emails. The departmental printer jammed again, and now I'm late for a meeting. The cafeteria ran out of the good snacks, so I'm surviving on vending machine chips. Sleep has become a luxury I can no longer afford. - Similarity: 0.0000\n",
      "Document 1: Sleepless nights in the lab have become my new normal. I tried to fix the experiment setup, but the apparatus seems to have a mind of its own. My advisor says results are just around the corner, but the corner keeps moving. Coffee is my only true companion these days. - Similarity: 0.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Updated Corpus\n",
    "corpus = [\n",
    "    \"Sleepless nights in the lab have become my new normal. I tried to fix the experiment setup, but the apparatus seems to have a mind of its own. My advisor says results are just around the corner, but the corner keeps moving. Coffee is my only true companion these days.\",\n",
    "    \"I thought grad school would be intellectually stimulating, but it's mostly paperwork and waiting for emails. The departmental printer jammed again, and now I'm late for a meeting. The cafeteria ran out of the good snacks, so I'm surviving on vending machine chips. Sleep has become a luxury I can no longer afford.\",\n",
    "    \"Writing the dissertation feels like climbing an endless mountain. Every time I finish a chapter, my supervisor suggests new revisions. The impostor syndrome is real, and I wonder if they made a mistake accepting me. Maybe I should have gone to clown college instead. I am utterly deprived of any semblance of a normal life.\",\n",
    "    \"My research data got corrupted, and now I have to start over. The lab mouse escaped, and we spent hours trying to find it. The grant proposal deadline is tomorrow, and the online submission portal is down. At least my pet cactus hasn't died yet.\",\n",
    "    \"The group meeting turned into a three-hour debate over font choices for the presentation. I'm pretty sure my colleague is stealing my lunch from the fridge. The photocopier is out to get me; it never works when I'm in a hurry. Is there a PhD in napping? Because I'd ace that.\",\n",
    "    \"I haven't seen the sun in days due to endless coding sessions. The simulation keeps crashing, and Stack Overflow doesn't have the answers. My roommate thinks I'm a ghost haunting the apartment. Instant noodles have become my primary food group.\",\n",
    "    \"Attending conferences sounded fun until I realized they involve a lot of awkward networking. I accidentally spilled coffee on a famous professor's shoes. My poster fell down twice during the session. Next time, I'll just send a cardboard cutout of myself.\",\n",
    "    \"The university gym membership was supposed to keep me healthy, but I've only used it once. I tried to attend a yoga class after staying up late for a deadline, but I fell asleep during the meditation. Maybe instead of the gym, my bed is more essential for keeping me healthy.\",\n",
    "    \"My teaching assistantship involves grading endless stacks of exams. Students keep emailing me for extensions with creative excuses. One claimed their dog sleeps on the laptop so they cannot use it for the exam. I was deprived of excuses for not completing my dissertation draft, and I might have got some good ones.\",\n",
    "    \"Group projects are the worst when you're the only one doing the work. My team members are as elusive as Bigfoot. The project is due next week, and I haven't heard from them. Perhaps I should just write a paper on the sociological implications of group work avoidance.\"\n",
    "]\n",
    "\n",
    "## 定義相同的查詢\n",
    "query = [\"sleep deprivation\"]\n",
    "\n",
    "# Unigram TF-IDF (單詞級別)\n",
    "vectorizer_unigram = TfidfVectorizer(ngram_range=(1, 1))\n",
    "X_unigram = vectorizer_unigram.fit_transform(corpus)\n",
    "query_vec_unigram = vectorizer_unigram.transform(query)\n",
    "\n",
    "# Bigram TF-IDF (二元詞組級別)\n",
    "vectorizer_bigram = TfidfVectorizer(ngram_range=(2, 2))\n",
    "X_bigram = vectorizer_bigram.fit_transform(corpus)\n",
    "query_vec_bigram = vectorizer_bigram.transform(query)\n",
    "\n",
    "# 計算餘弦相似度\n",
    "cos_sim_unigram = cosine_similarity(query_vec_unigram, X_unigram)\n",
    "cos_sim_bigram = cosine_similarity(query_vec_bigram, X_bigram)\n",
    "\n",
    "# 排序結果\n",
    "ranking_unigram = cos_sim_unigram[0].argsort()[::-1]\n",
    "ranking_bigram = cos_sim_bigram[0].argsort()[::-1]\n",
    "\n",
    "# 顯示Unigram結果\n",
    "print(\"Unigram TF-IDF Document ranking based on query similarity:\\n\", ranking_unigram)\n",
    "for i in ranking_unigram:\n",
    "    print(f\"Document {i + 1}: {corpus[i]} - Similarity: {cos_sim_unigram[0][i]:.4f}\")\n",
    "\n",
    "# 顯示Bigram結果\n",
    "print(\"\\nBigram TF-IDF Document ranking based on query similarity:\\n\", ranking_bigram)\n",
    "for i in ranking_bigram:\n",
    "    print(f\"Document {i + 1}: {corpus[i]} - Similarity: {cos_sim_bigram[0][i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232e315b-b61e-41b0-b73f-72ba9f341cd4",
   "metadata": {},
   "source": [
    "## Part 2: Analyze The Results from TF-IDF, Bigram TF-IDF, Word2Vec, and BERT. \n",
    "Do they successfully retrieve the relevant documents? Compare these four methods using **quantitative** (metrics we introduces in W3) and **qualitative** (case study) analysis.\n",
    "You can write your own code to compute the quantitative evaluation metrics, or use packages such as scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a8356fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Document ranking based on query similarity: [9 3 6 2 5 7 4 8 0 1]\n",
      "Document 10: Group projects are the worst when you're the only one doing the work. My team members are as elusive as Bigfoot. The project is due next week, and I haven't heard from them. Perhaps I should just write a paper on the sociological implications of group work avoidance. - Similarity: 0.1433\n",
      "Document 4: My research data got corrupted, and now I have to start over. The lab mouse escaped, and we spent hours trying to find it. The grant proposal deadline is tomorrow, and the online submission portal is down. At least my pet cactus hasn't died yet. - Similarity: 0.0752\n",
      "Document 7: Attending conferences sounded fun until I realized they involve a lot of awkward networking. I accidentally spilled coffee on a famous professor's shoes. My poster fell down twice during the session. Next time, I'll just send a cardboard cutout of myself. - Similarity: 0.0670\n",
      "Document 3: Writing the dissertation feels like climbing an endless mountain. Every time I finish a chapter, my supervisor suggests new revisions. The impostor syndrome is real, and I wonder if they made a mistake accepting me. Maybe I should have gone to clown college instead. I am utterly deprived of any semblance of a normal life. - Similarity: 0.0577\n",
      "Document 6: I haven't seen the sun in days due to endless coding sessions. The simulation keeps crashing, and Stack Overflow doesn't have the answers. My roommate thinks I'm a ghost haunting the apartment. Instant noodles have become my primary food group. - Similarity: 0.0526\n",
      "Document 8: The university gym membership was supposed to keep me healthy, but I've only used it once. I tried to attend a yoga class after staying up late for a deadline, but I fell asleep during the meditation. Maybe instead of the gym, my bed is more essential for keeping me healthy. - Similarity: 0.0398\n",
      "Document 5: The group meeting turned into a three-hour debate over font choices for the presentation. I'm pretty sure my colleague is stealing my lunch from the fridge. The photocopier is out to get me; it never works when I'm in a hurry. Is there a PhD in napping? Because I'd ace that. - Similarity: -0.0015\n",
      "Document 9: My teaching assistantship involves grading endless stacks of exams. Students keep emailing me for extensions with creative excuses. One claimed their dog sleeps on the laptop so they cannot use it for the exam. I was deprived of excuses for not completing my dissertation draft, and I might have got some good ones. - Similarity: -0.0069\n",
      "Document 1: Sleepless nights in the lab have become my new normal. I tried to fix the experiment setup, but the apparatus seems to have a mind of its own. My advisor says results are just around the corner, but the corner keeps moving. Coffee is my only true companion these days. - Similarity: -0.0728\n",
      "Document 2: I thought grad school would be intellectually stimulating, but it's mostly paperwork and waiting for emails. The departmental printer jammed again, and now I'm late for a meeting. The cafeteria ran out of the good snacks, so I'm surviving on vending machine chips. Sleep has become a luxury I can no longer afford. - Similarity: -0.1157\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Step 1: Tokenize the corpus\n",
    "corpus_tokenized = [doc.lower().split() for doc in corpus]\n",
    "\n",
    "# Step 2: Train Word2Vec model\n",
    "model = Word2Vec(corpus_tokenized, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Step 3: Calculate the query vector (filter out words not in model's vocabulary)\n",
    "query_vec = np.mean([model.wv[word] for word in query[0].split() if word in model.wv], axis=0)\n",
    "\n",
    "# Step 4: Calculate document vectors (also filter out words not in model's vocabulary)\n",
    "doc_vecs = np.array([np.mean([model.wv[word] for word in doc.split() if word in model.wv], axis=0) \n",
    "                     if len([word for word in doc.split() if word in model.wv]) > 0 else np.zeros(100) \n",
    "                     for doc in corpus])\n",
    "\n",
    "# Step 5: Compute cosine similarity and rank documents\n",
    "cos_sim = cosine_similarity([query_vec], doc_vecs)\n",
    "\n",
    "# Step 6: Rank documents based on similarity\n",
    "ranking = cos_sim[0].argsort()[::-1]\n",
    "print(\"Word2Vec Document ranking based on query similarity:\", ranking)\n",
    "\n",
    "# Display the ranking results\n",
    "for i in ranking:\n",
    "    print(f\"Document {i + 1}: {corpus[i]} - Similarity: {cos_sim[0][i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e7e5bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/IR_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Document ranking based on query similarity: [5 7 0 1 4 6 2 3 9 8]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "inputs = tokenizer(corpus, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "query_inputs = tokenizer(query, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "\n",
    "with torch.no_grad():\n",
    "    doc_embeddings = model(**inputs).last_hidden_state.mean(dim=1)\n",
    "    query_embedding = model(**query_inputs).last_hidden_state.mean(dim=1)\n",
    "\n",
    "cos_sim = cosine_similarity(query_embedding, doc_embeddings)\n",
    "ranking = cos_sim[0].argsort()[::-1]\n",
    "print(\"BERT Document ranking based on query similarity:\", ranking)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e027507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6667, Recall: 0.2857, F1-Score: 0.4000\n",
      "Mean Average Precision (MAP): 0.7857\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, average_precision_score\n",
    "\n",
    "true_labels = [1, 1, 0, 1, 1, 1, 1, 0, 1, 0] \n",
    "\n",
    "predicted_scores = cos_sim[0]\n",
    "\n",
    "threshold = sorted(predicted_scores, reverse=True)[2]\n",
    "predicted_labels = [1 if score >= threshold else 0 for score in predicted_scores]\n",
    "\n",
    "precision = precision_score(true_labels, predicted_labels)\n",
    "recall = recall_score(true_labels, predicted_labels)\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
    "\n",
    "map_score = average_precision_score(true_labels, predicted_scores)\n",
    "print(f\"Mean Average Precision (MAP): {map_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "369ef2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e0c05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "jupyter nbconvert --to html M124111043_MIS590_W6_Lab_Exercise.ipynb\n",
    "jupyter nbconvert --to html --execute --TemplateExporter.exclude_input=True M124111043_MIS590_W6_Lab_Exercise.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c642674-534b-4ce3-b5d7-3d0f2a22ebe8",
   "metadata": {},
   "source": [
    "## 💻 Assignment Submission 💻 \n",
    "Write your code and display the results in this Jupyter Notebook. Then, export it as an HTML file and submit both the Jupyter Notebook and the HTML file to Cyber University. </br>\n",
    "**Please ensure that the code is executed and the outputs are visible when exporting the HTML file.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IR_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
